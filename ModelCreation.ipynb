{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import Perceptron, LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying Different Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "non_normalized_dataset = pd.read_csv('non-normalized data.csv')\n",
    "normalized_dataset = pd.read_csv('normalized data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into labels and data\n",
    "normalized_labels = normalized_dataset.pop('Request Status').values\n",
    "normalized_data = normalized_dataset.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the KFold Object.\n",
    "kf = KFold(n_splits=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_result_dict(name, model):\n",
    "    \"\"\"\n",
    "      Creates a dictionary that can hold the results for the model\n",
    "\n",
    "      Args:\n",
    "        name: name of the model being classified\n",
    "        model: model being classified\n",
    "    \"\"\"\n",
    "    return {\n",
    "        'name' : name,\n",
    "        'model' : model,\n",
    "        'training_times' : [],\n",
    "        'average_training_time' : None,\n",
    "        'testing_times' : [],\n",
    "        'average_testing_time' : None,\n",
    "        'accuracies' : [],\n",
    "        'average_accuracy' : None\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding each model I want to test with their parameters\n",
    "models = [\n",
    "    create_result_dict('Random Forest', RandomForestClassifier),\n",
    "    create_result_dict('Perceptron', Perceptron),\n",
    "    create_result_dict('Logistic Regression', LogisticRegression),\n",
    "    create_result_dict('MultiLayer Perceptron', MLPClassifier)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'Random Forest',\n",
       "  'model': sklearn.ensemble._forest.RandomForestClassifier,\n",
       "  'training_times': [5.921321392059326,\n",
       "   5.7857208251953125,\n",
       "   5.743280410766602,\n",
       "   5.737278938293457,\n",
       "   5.691267251968384,\n",
       "   5.7722859382629395,\n",
       "   5.802292585372925,\n",
       "   5.746282577514648,\n",
       "   5.810157775878906,\n",
       "   5.841301679611206],\n",
       "  'average_training_time': None,\n",
       "  'testing_times': [0.08101701736450195,\n",
       "   0.0800175666809082,\n",
       "   0.07701683044433594,\n",
       "   0.07901716232299805,\n",
       "   0.08001852035522461,\n",
       "   0.0780179500579834,\n",
       "   0.0800180435180664,\n",
       "   0.08301854133605957,\n",
       "   0.07901763916015625,\n",
       "   0.07901740074157715],\n",
       "  'average_testing_time': None,\n",
       "  'accuracies': [0.8921952492821718,\n",
       "   0.8885408509527538,\n",
       "   0.8898459932132603,\n",
       "   0.8835813103628295,\n",
       "   0.8885408509527538,\n",
       "   0.8841033672670321,\n",
       "   0.8825065274151436,\n",
       "   0.8892950391644908,\n",
       "   0.889556135770235,\n",
       "   0.8900783289817232],\n",
       "  'average_accuracy': None},\n",
       " {'name': 'Perceptron',\n",
       "  'model': sklearn.linear_model._perceptron.Perceptron,\n",
       "  'training_times': [0.014003753662109375,\n",
       "   0.021004676818847656,\n",
       "   0.01600360870361328,\n",
       "   0.015002965927124023,\n",
       "   0.017003297805786133,\n",
       "   0.019004106521606445,\n",
       "   0.0200042724609375,\n",
       "   0.03900957107543945,\n",
       "   0.019004344940185547,\n",
       "   0.01800394058227539],\n",
       "  'average_training_time': None,\n",
       "  'testing_times': [0.0009996891021728516,\n",
       "   0.0009996891021728516,\n",
       "   0.0010004043579101562,\n",
       "   0.0010004043579101562,\n",
       "   0.0010004043579101562,\n",
       "   0.0010004043579101562,\n",
       "   0.001001119613647461,\n",
       "   0.0009992122650146484,\n",
       "   0.0010008811950683594,\n",
       "   0.001001119613647461],\n",
       "  'average_testing_time': None,\n",
       "  'accuracies': [0.11981205951448708,\n",
       "   0.8835813103628295,\n",
       "   0.1331245105716523,\n",
       "   0.5614722004698512,\n",
       "   0.8780997128687027,\n",
       "   0.8666144609762464,\n",
       "   0.4840731070496084,\n",
       "   0.7953002610966058,\n",
       "   0.8887728459530027,\n",
       "   0.8845953002610966],\n",
       "  'average_accuracy': None},\n",
       " {'name': 'Logistic Regression',\n",
       "  'model': sklearn.linear_model._logistic.LogisticRegression,\n",
       "  'training_times': [0.08801960945129395,\n",
       "   0.10302305221557617,\n",
       "   0.0780174732208252,\n",
       "   0.09402132034301758,\n",
       "   0.11902689933776855,\n",
       "   0.11702609062194824,\n",
       "   0.11902713775634766,\n",
       "   0.10802435874938965,\n",
       "   0.10502362251281738,\n",
       "   0.11202359199523926],\n",
       "  'average_training_time': None,\n",
       "  'testing_times': [0.0,\n",
       "   0.0010004043579101562,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0010027885437011719,\n",
       "   0.0009999275207519531,\n",
       "   0.0009999275207519531,\n",
       "   0.0,\n",
       "   0.0009999275207519531,\n",
       "   0.0],\n",
       "  'average_testing_time': None,\n",
       "  'accuracies': [0.8898459932132603,\n",
       "   0.8841033672670321,\n",
       "   0.8872357086922474,\n",
       "   0.8788827982250065,\n",
       "   0.8841033672670321,\n",
       "   0.881232054293918,\n",
       "   0.8746736292428199,\n",
       "   0.8864229765013055,\n",
       "   0.8887728459530027,\n",
       "   0.8882506527415144],\n",
       "  'average_accuracy': None},\n",
       " {'name': 'MultiLayer Perceptron',\n",
       "  'model': sklearn.neural_network._multilayer_perceptron.MLPClassifier,\n",
       "  'training_times': [5.99309515953064,\n",
       "   7.849748849868774,\n",
       "   8.002782821655273,\n",
       "   10.649373054504395,\n",
       "   6.018340349197388,\n",
       "   5.555988073348999,\n",
       "   6.3064045906066895,\n",
       "   3.435765504837036,\n",
       "   5.44121241569519,\n",
       "   8.240835905075073],\n",
       "  'average_training_time': None,\n",
       "  'testing_times': [0.0030007362365722656,\n",
       "   0.0030002593994140625,\n",
       "   0.0030012130737304688,\n",
       "   0.0030007362365722656,\n",
       "   0.003000497817993164,\n",
       "   0.0030002593994140625,\n",
       "   0.0030019283294677734,\n",
       "   0.0030002593994140625,\n",
       "   0.003000497817993164,\n",
       "   0.0030002593994140625],\n",
       "  'average_testing_time': None,\n",
       "  'accuracies': [0.8898459932132603,\n",
       "   0.8841033672670321,\n",
       "   0.8872357086922474,\n",
       "   0.8788827982250065,\n",
       "   0.8841033672670321,\n",
       "   0.881232054293918,\n",
       "   0.8746736292428199,\n",
       "   0.8864229765013055,\n",
       "   0.8887728459530027,\n",
       "   0.8882506527415144],\n",
       "  'average_accuracy': None}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train each model over each fold in the kfold and gather their performance\n",
    "for train_index, test_index in kf.split(normalized_data):\n",
    "    # For each model, create a new one and gather information about its performance\n",
    "    for model in models:\n",
    "        fresh_model = model['model']()\n",
    "        # Train the model\n",
    "        initial_train_time = time.time()\n",
    "        fresh_model.fit(normalized_data[train_index], normalized_labels[train_index])\n",
    "        model['training_times'].append(time.time() - initial_train_time)\n",
    "        # Test the model\n",
    "        initial_test_time = time.time()\n",
    "        predicted_labels = fresh_model.predict(normalized_data[test_index])\n",
    "        model['testing_times'].append(time.time() - initial_test_time)\n",
    "        model['accuracies'].append(accuracy_score(normalized_labels[test_index], predicted_labels))\n",
    "\n",
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Random Forest', 'model': <class 'sklearn.ensemble._forest.RandomForestClassifier'>, 'training_times': [5.921321392059326, 5.7857208251953125, 5.743280410766602, 5.737278938293457, 5.691267251968384, 5.7722859382629395, 5.802292585372925, 5.746282577514648, 5.810157775878906, 5.841301679611206], 'average_training_time': 5.785118937492371, 'testing_times': [0.08101701736450195, 0.0800175666809082, 0.07701683044433594, 0.07901716232299805, 0.08001852035522461, 0.0780179500579834, 0.0800180435180664, 0.08301854133605957, 0.07901763916015625, 0.07901740074157715], 'average_testing_time': 0.07961766719818116, 'accuracies': [0.8921952492821718, 0.8885408509527538, 0.8898459932132603, 0.8835813103628295, 0.8885408509527538, 0.8841033672670321, 0.8825065274151436, 0.8892950391644908, 0.889556135770235, 0.8900783289817232], 'average_accuracy': 0.8878243653362394}\n",
      "{'name': 'Perceptron', 'model': <class 'sklearn.linear_model._perceptron.Perceptron'>, 'training_times': [0.014003753662109375, 0.021004676818847656, 0.01600360870361328, 0.015002965927124023, 0.017003297805786133, 0.019004106521606445, 0.0200042724609375, 0.03900957107543945, 0.019004344940185547, 0.01800394058227539], 'average_training_time': 0.01980445384979248, 'testing_times': [0.0009996891021728516, 0.0009996891021728516, 0.0010004043579101562, 0.0010004043579101562, 0.0010004043579101562, 0.0010004043579101562, 0.001001119613647461, 0.0009992122650146484, 0.0010008811950683594, 0.001001119613647461], 'average_testing_time': 0.0010003328323364257, 'accuracies': [0.11981205951448708, 0.8835813103628295, 0.1331245105716523, 0.5614722004698512, 0.8780997128687027, 0.8666144609762464, 0.4840731070496084, 0.7953002610966058, 0.8887728459530027, 0.8845953002610966], 'average_accuracy': 0.6495445769124082}\n",
      "{'name': 'Logistic Regression', 'model': <class 'sklearn.linear_model._logistic.LogisticRegression'>, 'training_times': [0.08801960945129395, 0.10302305221557617, 0.0780174732208252, 0.09402132034301758, 0.11902689933776855, 0.11702609062194824, 0.11902713775634766, 0.10802435874938965, 0.10502362251281738, 0.11202359199523926], 'average_training_time': 0.10432331562042237, 'testing_times': [0.0, 0.0010004043579101562, 0.0, 0.0, 0.0010027885437011719, 0.0009999275207519531, 0.0009999275207519531, 0.0, 0.0009999275207519531, 0.0], 'average_testing_time': 0.0005002975463867188, 'accuracies': [0.8898459932132603, 0.8841033672670321, 0.8872357086922474, 0.8788827982250065, 0.8841033672670321, 0.881232054293918, 0.8746736292428199, 0.8864229765013055, 0.8887728459530027, 0.8882506527415144], 'average_accuracy': 0.8843523393397138}\n",
      "{'name': 'MultiLayer Perceptron', 'model': <class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>, 'training_times': [5.99309515953064, 7.849748849868774, 8.002782821655273, 10.649373054504395, 6.018340349197388, 5.555988073348999, 6.3064045906066895, 3.435765504837036, 5.44121241569519, 8.240835905075073], 'average_training_time': 6.749354672431946, 'testing_times': [0.0030007362365722656, 0.0030002593994140625, 0.0030012130737304688, 0.0030007362365722656, 0.003000497817993164, 0.0030002593994140625, 0.0030019283294677734, 0.0030002593994140625, 0.003000497817993164, 0.0030002593994140625], 'average_testing_time': 0.0030006647109985353, 'accuracies': [0.8898459932132603, 0.8841033672670321, 0.8872357086922474, 0.8788827982250065, 0.8841033672670321, 0.881232054293918, 0.8746736292428199, 0.8864229765013055, 0.8887728459530027, 0.8882506527415144], 'average_accuracy': 0.8843523393397138}\n"
     ]
    }
   ],
   "source": [
    "# Compute the averages\n",
    "for model in models:\n",
    "    model['average_training_time'] = sum(model['training_times'])/len(model['training_times'])\n",
    "    model['average_testing_time'] = sum(model['testing_times'])/len(model['testing_times'])\n",
    "    model['average_accuracy'] = sum(model['accuracies'])/len(model['accuracies'])\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate Averages with Cross Validation, find times it takes to train and run each model with it as this is a time sensitive task. A prediction system that is 100% accurate but takes a year to predict is worthless when you have 10 minutes until the rider shoes up for the ride"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider 3 things about model, train time, run time, and accuracy. why? Well presumably this dataset expands year after year, and they could either want to A: make a new model on last years data or B: train a model/update it over all current data. If this update is done every month, then a slow train time could mean the software is down for hours and not worthy. Accuracy is obvious, an inaccurate model is useless. train time seeabove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, the times for random forest are a bit skewed. Every other model requires scaling and normalizing, but random forest wouldn't need such things and thus could run faster. However, they all do need to changing and cleaning of data to only what is required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bda4b62ebfd68aeb3321c302634d1d29c33c71a8a620b15e2fdb9d843cc1ece8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
